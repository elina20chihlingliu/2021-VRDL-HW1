# -*- coding: utf-8 -*-
"""feature_nowwin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J8cTe-mM8PR7WpStX3llOY4dIp3Kievm
"""

# for garbage collection
import gc

# for warnings
import warnings
warnings.filterwarnings("ignore")

import tqdm

# utility libraries
import os
import numpy as np 
import pandas as pd 
import cv2
import tensorflow as tf

# keras libraries
import keras
from tensorflow.keras.utils import to_categorical
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Model
from keras.layers import BatchNormalization, Dense, GlobalAveragePooling2D, Lambda, Dropout, InputLayer, Input
from keras import backend as K
from keras.callbacks import EarlyStopping
from sklearn.model_selection import StratifiedKFold
from keras import optimizers

data_dir = os.path.abspath(os.getcwd())
print("The directory of the executable file is "+data_dir)

train_txt = '/training_labels.txt'
test_txt = '/testing_img_order.txt'

def set_labeldf():
  header_list = ["img", "breed"]
  labels_df  = pd.read_csv(data_dir + train_txt, sep = " ",names = header_list)  # all the testing images
  train_dir = 'training_images/'
  bird_breeds = sorted(list(set(labels_df['breed'])))
  n_classes = len(bird_breeds)

  class_dict = dict(zip(bird_breeds, range(n_classes)))
  labels_df['file_path'] = labels_df['img'].apply(lambda x:train_dir+f"{x}")
  labels_df['breed'] = labels_df.breed.map(class_dict)
  return labels_df, class_dict
labels_dataframe, class_to_num = set_labeldf()

# set image size here
img_size = 500
bath_size = 64


# TRAIN IMAGES
def read_trainimg(data):
  X_list = []
  for image in tqdm.tqdm(data):
      # Reading RGB Images
      image_path = data_dir+'/'+image
      print(image_path)
      orig_image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)
      res_image = cv2.resize(orig_image,(img_size, img_size))
      X_list.append(res_image)
  return X_list

X_ls = read_trainimg(labels_dataframe['file_path'])

# Converting to arrays
Xarr = np.array(X_ls)
y_train = to_categorical(labels_dataframe.breed)
Y = list(labels_dataframe.breed)

del(X_ls)
gc.collect()
print(Xarr.shape, y_train.shape)


# TEST IMAGES
with open(data_dir+test_txt) as f:
      test_images = [x.strip() for x in f.readlines()]  # all the testing images
def read_testimg():
  test_dir = 'testing_images/'
  f.close()
  X_tsls = [] 
  for image in tqdm.tqdm(np.array(test_images)):
    image_path = data_dir+'/'+test_dir+image
    orig_image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)
    res_image = cv2.resize(orig_image,(img_size, img_size))
    X_tsls.append(res_image)
  return X_tsls

X_test = read_testimg()

# Converting to arrays
Xtesarr = np.array(X_test)
del(X_test)
gc.collect()
print(Xtesarr.shape)



# FEATURE EXTRACTION OF TRAINING ARRAYS
AUTO = tf.data.experimental.AUTOTUNE
def get_features(model_name, data_preprocessor, data):
    '''
    1- Create a feature extractor to extract features from the data.
    2- Returns the extracted features and the feature extractor.

    '''
    dataset = tf.data.Dataset.from_tensor_slices(data)
    
    # data augmentation
    def preprocess(x):
        x = tf.image.random_flip_left_right(x)
        return x

    ds = dataset.map(preprocess, num_parallel_calls=AUTO).batch(bath_size)

    input_size = data.shape[1:]
    
    # Prepare pipeline.
    input_layer = Input(input_size)
    preprocessor = Lambda(data_preprocessor)(input_layer)

    base_model = model_name(weights='imagenet', include_top=False,
                 input_shape=input_size)(preprocessor)

    avg = GlobalAveragePooling2D()(base_model)
    feature_extractor = Model(inputs = input_layer, outputs = avg)


    # Extract feature.
    feature_maps = feature_extractor.predict(ds, verbose=1)
    print('Feature maps shape: ', feature_maps.shape)
    
    # deleting variables
    del(feature_extractor, base_model, preprocessor, dataset)
    gc.collect()
    return feature_maps


# FEATURE EXTRACTION OF VALIDAION AND TESTING ARRAYS
def get_valfeatures(model_name, data_preprocessor, data):
    '''
    Same as above except not image augmentations applied.
    Used for feature extraction of validation and testing.
    '''
    dataset = tf.data.Dataset.from_tensor_slices(data)

    ds = dataset.batch(bath_size)

    input_size = data.shape[1:]

    # Prepare pipeline.
    input_layer = Input(input_size)

    preprocessor = Lambda(data_preprocessor)(input_layer)

    base_model = model_name(weights = 'imagenet', include_top = False,
                input_shape = input_size)(preprocessor)
                
    avg = GlobalAveragePooling2D()(base_model)
    feature_extractor = Model(inputs = input_layer, outputs = avg)

    # Extract feature.
    feature_maps = feature_extractor.predict(ds, verbose=1)
    print('Feature maps shape: ', feature_maps.shape)
    return feature_maps

# RETURNING CONCATENATED FEATURES USING MODELS AND PREPROCESSORS
def get_concat_features(feat_func, models, preprocs, array):

    print(f"Beggining extraction with {feat_func.__name__}\n")
    feats_list = []

    for i in range(len(models)):
        
        print(f"\nStarting feature extraction with {models[i].__name__} using {preprocs[i].__name__}\n")
        
        # applying the above function and storing in list
        feats_list.append(feat_func(models[i], preprocs[i], array))

    # features concatenating
    final_feats = np.concatenate(feats_list, axis=-1)
    
    # memory saving
    del(feats_list, array)
    gc.collect()

    return final_feats

# DEFINING models and preprocessors imports 
from keras.applications.xception import Xception, preprocess_input #0.57
xception_preprocessor = preprocess_input

from keras.applications.resnet_v2 import ResNet152V2, preprocess_input 
resnext_preprocessor = preprocess_input
'''
from keras.applications.nasnet import NASNetLarge, preprocess_input #0.37
nasnet_preprocessor = preprocess_input

from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input #0.49
inc_resnet_preprocessor = preprocess_input

from keras.applications.inception_v3 import InceptionV3, preprocess_input #0.49
inception_preprocessor = preprocess_input  
'''

models = [ResNet152V2, Xception]
preprocs = [resnext_preprocessor, xception_preprocessor]

# calculating features of train the data
final_train_features = get_concat_features(get_features, models, preprocs, Xarr)
print('Final feature maps shape', final_train_features.shape)

EarlyStop_callback = keras.callbacks.EarlyStopping(monitor = 'val_accuracy', 
                                patience = 10, 
                                restore_best_weights=True,
                                verbose = 0)

my_callback=[EarlyStop_callback]

splits = list(StratifiedKFold(n_splits = 5, shuffle=True, random_state=10).split(final_train_features, Y))

# set lists of trained_models, accuracy, losses
trained_models = []
accuracy = []
losses = []

# Prepare And Train DNN model
for i, (train_idx, valid_idx) in enumerate(splits): 

    print(f"\nStarting fold {i+1}\n")
    x_train_fold = final_train_features[train_idx, :]
    y_train_fold = y_train[train_idx, :]
    x_val_fold = final_train_features[valid_idx]
    y_val_fold = y_train[valid_idx, :]

    dnn = keras.models.Sequential([
        InputLayer(final_train_features.shape[1:]),
        Dropout(0.7),
        Dense(200, activation='softmax')
    ])
    
    optimizer = tf.keras.optimizers.Adam(lr = 0.001) # 0.001
    dnn.compile(optimizer = optimizer, #'adam'
                loss = 'categorical_crossentropy',
                metrics = ['accuracy'])
    print("Training...")
    
    #Train simple DNN on extracted features.
    h = dnn.fit(x_train_fold, y_train_fold,
                batch_size = bath_size, #64
                epochs = 80,
                verbose = 2,
                validation_data = (x_val_fold, y_val_fold),
                callbacks = my_callback)  

    print("Evaluating model ...")
    model_res = dnn.evaluate(x_val_fold, y_val_fold)

    accuracy.append(model_res[1])
    losses.append(model_res[0])
    trained_models.append(dnn)

print('\n CV Score -')
print(f"\nAccuracy - {sum(accuracy)/len(accuracy)}")
print(f"\nLoss - {sum(losses)/len(losses)}")

# save five train models
# def model_save():
#   for i in range (len(trained_models)):
#     trained_models[i].save(data_dir+"/my_model"+str(i))
# model_save()

# # load five models
# trained_models = []
# def model_load():
#   for i in range (5):
#     trained_models.append(keras.models.load_model(data_dir+"/my_model"+str(i)))
# model_load()


# calculating features of test the data
test_features = get_concat_features(get_valfeatures, models, preprocs, Xtesarr)

del(Xtesarr)
gc.collect()
print('Final feature maps shape', test_features.shape)

def predict(models, test_fea):
  y_pred_norm = models[0].predict(test_fea, batch_size = bath_size)/3
  for dnn in models[1:]:
      y_pred_norm += dnn.predict(test_fea, batch_size = bath_size)/3
  
  dict2 = {value:key for key, value in class_to_num.items()}
  pred_codes = np.argmax(y_pred_norm, axis = 1)
  result = pd.Series(pred_codes).map(dict2)
  return result

answer = predict(trained_models, test_features)
print(answer)

submission = np.array(pd.concat([pd.Series(test_images), answer], axis=1))
np.savetxt(data_dir+'/answer.txt', submission, fmt='%s')



